<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">
		
		<!-- BASE URL: change this href to the intended URL of your slide show, e.g. - http://USERNAME.github.io/SLIDESHOW-REPO/ -->
		<base href="http://millsjb.github.io/inls161-revealjs-template//"> 

		<title>In-Car UX</title>
		
		
		<meta name="author" content="Josh Mills">
		
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
		
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css" id="theme">
		<!-- Code syntax highlighting -->
  		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
		
		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				
				<section>
					<h2>In-Car UX</h2>
					<p>Josh Mills</p>
					<p>
						<h4>2016-01-28</h4>
					</p>
				</section>
				
<section class="slide level1">
<p>Research Area</p>
<aside class="notes">
Head-mounted display have potential to improve the current situation of car drivers. With the system that was 
built to accommodate a (HMD), a dual task study was conducted in a driving simulation, where different techniques 
of content stabilization (head and cockpit stabilized visualizations) were compared. 
</aside>
</section>

<section class="slide level1">
<p>Cars as a personal space</p>
<aside class="notes">
Cars have now become a personal space in which drivers want to relax and use their accustomed channels of 
communication and a comprehensive amount of infotainment systems. 
</aside>
</section>

<section class="slide level1">
<p>The challenge</p>
<aside class="notes">
As a result, introducing more and more functionality 
into the car while minimizing the negative effects on the driving task is one of the main challenges for today’s car manufacturers.  
</aside>
</section>

<section class="slide level1">
<p>Drawbacks</p>
<aside class="notes">
One of the major drawbacks for most in-vehicle displays is that they require the driver to avert his/her eyes away from the street. 
Therefore, the purpose of this study is to show/prove that (HMD’s) can overcome these disadvantages s they provide a very 
flexible FOV where content can be displayed regardless of the driver’s line of vision, while at the same time preserving all of 
the advantages of a (HUD) system
</aside>
</section>

<section class="slide level1">
<p>HMD's</p>
<a href="#">
	<img src="http://www.drivesquare.com/img/052307_Quantico_011.jpg" 
	alt="Ref image" style="width:80%;height:300px;">
</a>
<aside class="notes">
Explain/show HMD's
</aside>
</section>

<section class="slide level1">
<p>HUD's</p>
<a href="#">
	<img src="http://www.telematicsnews.info/wp-content/uploads/general/Conti%20HUD%202.jpg" 
	alt="Ref image" style="width:80%;height:300px;">
</a>
<aside class="notes">
Explain/show HUD's
</aside>
</section>

<section class="slide level1">
<p>Questions about HMD's</p>
<aside class="notes">
The main question I have about HMD's concerns their practicality. Why would users/consumers opt for an obtrusive HMD device when 
they could simply use an unobtrusive HUD?
</aside>
</section>

<section class="slide level1">
<p>Goals of the study</p>
<aside class="notes">
The first goal of the study was to examine the effect of the content stabilization technique on driving and 
secondary task performance
</aside>
</section>

<section class="slide level1">
<p>Variables</p>
<aside class="notes">
•	two different interaction modalities were compared (gestures and a physical controller)
•	two different content stabilization techniques (cockpit and head-stabilized)
•	4 use cases (audio control panel, a message center, incoming call and traffic notification center)
</aside>
</section>

<section class="slide level1">
<p>Analysis</p>
<aside class="notes">
•	Used a mixed 2x3 design for the study
•	So half used rotary controller while half used the hand gestures
•	Although, all used HMD head-stabilized, HMD cockpit stabilized, and the HUD as a baseline

</aside>
</section>

<section class="slide level1">
<p>Measuring performance</p>
<aside class="notes">
•	After each test drive, subjects were asked to complete two standardized questionnaires (System Usability Scale and NASA Raw Task Load Index)
•	Subjects also had to complete several scenario-specific 5-point Likert scales
</aside>
</section>

<section class="slide level1">
<p>Results</p>
<aside class="notes">
o	Generally the HUD condition tended to yield lower task completion times than the HMD conditions (significant in the phone task)
o	HUD yielded a significantly lower total task time than the HMD cockpit-stabilized condition and the HMD head-stabilized condition
o	In regard to the message task, the HMD head-stabilized condition yielded significantly lower task times than the HMD cockpit-stabilized condition
o	A significant effect of the stabilization techniques was found
</aside>
</section>

<section class="slide level1">
<p>Qualitatively..</p>
<aside class="notes">
•	The HUD was considered to be more comfortable to read than the HMD cockpit=stabilized and the HMD head-stabilized visualization.
•	Subjects agreed that HMD visualizations suffered from unsteady presentation cause by head movements
•	One subject stated that he would prefer vertical gestures, because horizontal gestures lead to unintentional steering movements
</aside>
</section>

<section class="slide level1">
<p>Practical implications</p>
<aside class="notes">
•	Surprised to find that the technology mature HUD did not outperform all HMD conditions, instead the best two SDLP values were achieved by HMD.
•	The difference is not significant and does not imply that one is safer
•	Regarding hand gestures, while longitudinal vehicle control did not significantly suffer from gesture interaction, there was a significant negative effect on the lateral control of the vehicle.
</aside>
</section>

<section class="slide level1">
<p>Applying the results</p>
<aside class="notes">
•	I think that the gesture part was significant because it showed how horizontal gestures might affect the also horizontal 
movement of turning
</aside>
</section>

<section class="slide level1">
<p>Questions?</p>
<aside class="notes">
</aside>
</section>
				
				
				
				
				
				

<section class="slide level1">
<p>Research Area</p>
<aside class="notes">
•	The advent of ultra-high resolution wall-size displays and their use for complex tasks require a more systematic analysis 
and deeper understanding of their advantages and drawbacks compared with desktop monitors. 
</aside>
</section>

<section class="slide level1">
<p>Is past research on desktop-sized displays still relavent?</p>
<aside class="notes">
Wall-size displays are becoming more common and therefore raising the question as to whether existing research findings on 
desktop-size displays still apply to this new environment.
</aside>
</section>

<section class="slide level1">
<a href="#">
	<img src="http://www.scsc.no/daniel/img/comics.jpg" 
	alt="Ref image" style="width:80%;height:300px;">
</a>
<aside class="notes">
Wall-sized displays offer a unique experience with many advantages  
</aside>
</section>

<section class="slide level1">
<p>Advantages:</p>
	
	<section class="level1">
	<p>The size of the data-set is the primary reason users want to move off of a desktop</p>
	</section>
	
	<section class="level1">
	<p>Virtual navigation imposed by a desktop interface can be disorienting, and the overhead of constantly 
	navigating the data set, e.g. with pan-and-zoom, can distract users and increase their cognitive load. </p>
	</section>
	
	<section class="level1">
	<p>Results suggest that large displays facilitate tasks with multiple windows and rich information 
	because they offer a more immersive experience, with enhanced peripheral awareness</p>
	</section>
	
	<section class="level1">
	<p>Larger displays promote physical navigation and improve user performance for search, navigation 
	and pattern-finding tasks</p>
	</section>
	
	
<aside class="notes">
large data-set, virtual navigation, more immersive experience, promote physical navigation
</aside>
</section>

<section class="level1">
<p>Even so, physical locomotion is more time-consuming and tiring than virtual navigation, and 
manipulating data with well-known devices and widgets may be more efficient</p>
</section>

<section class="slide level1">
<p>Variables</p>
<aside class="notes">
•	Control the complexity of our classification task via several parameters: number of items, number of classes, number of containers, and representation of the item (including the label font size)
•	We control the difficulty by setting the number of classes (Easy or Hard)
•	Use three levels for labelsize factor (small, medium, and large)
•	On the desktop, participants can pan and zoom the scene in order to read the labels, find the target container, and identify which item to move.
•	On the wall-size display, users stand or walk in front of the wall and use a tablet to control the cursor.

</aside>
</section>

<section class="slide level1">
<p>Reasons for choosing the variables</p>
<aside class="notes">
</aside>
</section>

<section class="slide level1">
<p>Exp 1:</p>
<aside class="notes">
Goal is to investigate the trade-offs between physical and virtual navigation and how they affect task performance
</aside>
</section>

<section class="slide level1">
<p>Three hypothesis:</p>
	<section class="level1">
	<p>Wall performs better than Desktop for smaller labels</p>
	</section>
	
	<section class="level1">
	<p>Wall performs better than Desktop for harder tasks</p>
	</section>
	
	<section class="level1">
	<p>Desktop performs better than Wall for larger labels and simpler tasks</p>
	</section>
</section>

<section class="slide level1">
<p>Exp 2:</p>
<aside class="notes">
Compared three destop techniques in a second expirement: the baseline pan-and-zoom technique, an overview+detail 
technique and a focus+context technique
</aside>
</section>

<section class="slide level1">
<p>Analysis</p>
<aside class="notes">
Participants filled out a questionaniare about their subjective workload at the end of each display condition 
and another about their preferences at the end
</aside>
</section>

<section class="slide level1">
<p>[2x3x2] within-participants design with three factors: (display, labelsize, and difficulty)</p>
</section>

<section class="slide level1">
<p>Results:</p>
	<section class="level1">
	<p>For large labels, Desktop is faster than Wall for both Easy and Hard difficulty</p>
	</section>
	
	<section class="level1">
	<p>For Medium labels there is no significant difference for Easy, but Wall is faster than Desktop for Hard</p>
	</section>
	
	<section class="level1">
	<p>For Small labels, Wall is faster than Desktop for both Easy and Hard</p>
	</section>
</section>

<p>Ultimately</p>
<aside class="notes">
•	As predicted, the desktop is faster for Large labels and the Wall is faster for Small labels
•	However, the magnitude of the difference depends on the difficulty
•	The absolute difference between Wall and Desktop in the Small-Hard condition is large, with the wall being about 35% faster
•	This supports the hypothesis that complex tasks become intractable on the desktop but still manageable on the wall-size display
•	EXP 2: The analysis of variance revealed no significant effect of technique on task completion time
</aside>
</section>

<section class="slide level1">
<p>Practical Implications</p>
<aside class="notes">
•	In summary though, the experiment confirmed that the wall-size display out performs the desktop for difficult data classification 
tasks
</aside>
</section>
		
<section class="slide level1">
<p>The in-car interface</p>
<aside class="notes">
What makes a good in-car interface? We are currently seeing a shift from a more traditional button and knob dashboard to one that is 
completely accessed/controlled through a touch screen.
</aside>
</section>	

<section class="slide level1">
<p>Affordances</p>
<aside class="notes">
While buttons and knobs may seem old-school and out-of-date, they do allow for important affordances that current touch screens 
cannot. For example, once a user is accustomed to the buttons, they are able to associate the way a certian button, lets say the volume 
knob for example, feels and therefore I/O may be done with little to no distraction to the driver. If a driver knows how the 
volume knob feels then they are able to change the volume without even looking away from the road.
</aside>
</section>

<section class="slide level1">
<a href="#">
	<img src="http://www.proclipusa.com/images/image.aspx/media/images/products/dashmounts/854352-4.JPG-800x">
</a>
<aside class="notes">
So here is my car actually, and we can see the volume button is of a different size than the other buttons in addition 
to being a different texture
</aside>
</section>

<section class="slide level1">
<p>Distraction</p>
<aside class="notes">
And as we know from kujala & Salvucci paper, anything distraction more than two seconds is dangerous
</aside>
</section>

<section class="slide level1">
<a href="#">
	<img src="http://blog.scottlogic.com/wloveland/assets/car-uis/tesla.jpg" 
	alt="Ref image" style="width:80%;height:80%;">
</a>
<aside class="notes">
Example of an in-car interface
</aside>
</section>

<section class="slide level1">
<a href="#">
	<img src="https://static-secure.guim.co.uk/sys-images/Observer/Pix/pictures/2015/1/5/1420473362163/Apples-CarPlay-012.jpg">
</a>
<aside class="notes">
Here you can see the interface acts much like traditional touch screen interfaces that we are familiar with such as an ipad 
or even more so, your phone. This is a good thing because it takes advantage of a design that we are both comfortable with 
and used to. However, this interface requires a large amount of attention and as (paper read for class about 2 second rule 
for driving) shows, looking away for any more than two seconds can be potentially dangerous/fatal. So how might we take advantage 
of such touch screens and gestures for in-car interfaces?
</aside>
</section>

<section class="slide level1">
<p>Proposal</p>
<aside class="notes">
So here is a proposal for the stated problem
</aside>
</section>

<section class="slide level1">
<aside class="notes">
So here is a proposal for the stated problem
</aside>
</section>	

<section data-background-iframe="https://www.youtube.com/embed/XVbuk3jizGM?start=20&end=122&version=3&modestBranding=1&autoplay=1&rel=0&amp;showinfo=0">
<aside class="notes">
Showing the video with the proposed UX solution
</aside>
</section>	

<section class="slide level1">
<p>Important concepts from video</p>
<aside class="notes">
So from the video we see an interface that has the ability to be used without much distraction to the user. The user can perform 
different operations simply by changing the number of fingers they use to interact with touch surface. Another important aspect is 
the forgivingness of the interface as it allows people to perform the same function by touching anywhere on the screen instead of 
standard point.
</aside>
</section>

<section class="slide level1">
<h3>Drivers tend to use only four of the UI features:</h3>
	
	<section class="slide level1">
	<p></p>
	</section>
	<!--possibly add some pics in place of listing the words-->
	<section class="slide level1">
	<a href="#">
		<img src="http://s3.motoringfile.com.s3.amazonaws.com/wp-content/uploads/2014/05/apple-carplay-phone-call.png" 
		alt="Ref image" style="width:80%;height:300px;">
	</a>
	<aside class="notes">
	phone calls
	</aside>
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="https://s-media-cache-ak0.pinimg.com/736x/f1/3a/0e/f13a0e659a0f95203ba9f6de89fa08b4.jpg" 
		alt="Ref image" style="width:80%;height:300px;">
	</a>
	<aside class="notes">
	music
	</aside>
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://i.imgur.com/FHxAnpj.png" 
		alt="Ref image" style="width:80%;height:300px;">
	</a>
	<aside class="notes">
	navigation
	</aside>
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://www.automonster.ca/wp-content/uploads/2010/12/rear-view-car-camera.jpg" 
		alt="Ref image" style="width:80%;height:300px;">
	</a>
	<aside class="notes">
	rear-view camera
	</aside>
	</section>
	
<aside class="notes">
According to www.hugeinc.com, they did a study and found that drivers tend to only use four UI features, while the others go widely unutilized.
</aside>
</section>

<section class="slide level1">
<p>From this..</p>
<aside class="notes">
From this we can glean that creating an interface where many options are available such as the typical ios interface is not necessary 
for in-car interfaces as drivers only use a couple of functions. Therefore, the proposed solution seen in the video would work great 
due to the fact that it only allows for a few functionality options due to the physical gesture limitations.
</aside>
</section>

<section class="slide level1">
<p>My suggestions</p>
<aside class="notes">
My suggestion would be either buttons or a touch screen interface such as the one we saw in a video that allow for affordances such as 
touch memory and/or gestures such as using different fingers for different functions as exemplified in the video. This will ultimately 
allow for a "hands-free" interface system where drivers do not have to avert their eyes from the road thus resulting in less distraction 
and hopefully less accidents
</aside>
</section>

<section class="slide level1">
<p>Works Cited</p>
<p>[1] www.hugeinc.com/ideas/perspective/riding-in-cars-with-uis</p>
<p>[2] https://www.youtube.com/watch?v=XVbuk3jizGM&list=LLunkFs4TPXfpIO88z8zLUBw&index=1</p>
<p>[3] Modeling Visual Sampling on In-Car Displays: The Challenge of Predicting Safety-Critical Lapses of Control</p>
<aside class="notes">
</aside>
</section>

<section class="slide level1">
<p>Questions?</p>
<aside class="notes">
</aside>
</section>



			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				overview: true,
				touch: true,
			

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				
				// USE THESE SETTINGS FOR THE AUDIO AND AUTO-ADVANCE
				//autoSlide: 5000,
				audioPrefix: 'audio/',        // audio files are stored in the "audio" folder
				audioSuffix: '.ogg',	      // audio files have the ".ogg" ending
				audioDefaultDuration: 2, // default duration if no audio is available
				audioPlayerOpacity: 0.05,     // opacity value of audio player if unfocused
				audioTextToSpeechURL: "http://tts-api.com/tts.mp3?q=",  // text is added to this URL
				
				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } }
				]
			});
		</script>

	</body>
</html>
				