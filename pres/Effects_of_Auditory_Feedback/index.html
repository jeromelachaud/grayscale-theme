<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">
		
		<!-- BASE URL: change this href to the intended URL of your slide show, e.g. - http://USERNAME.github.io/SLIDESHOW-REPO/ -->
		<base href="http://millsjb.github.io/inls161-revealjs-template//"> 

		<title>Effects of Auditory Feedback on Menu Selection in Hand-Gesture Interfaces</title>
		
		
		<meta name="author" content="Josh Mills">
		
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
		
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css" id="theme">
		<!-- Code syntax highlighting -->
  		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
		
		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				
				<section>
					<h2>Effects of Auditory Feedback on Menu Selection in Hand-Gesture Interfaces</h2>
					<p>Josh Mills</p>
					<p>
						<h4>2016-02-25</h4>
					</p>
				</section>

<section class="slide level1">
<p>Focus of this study</p>
<aside class="notes">
One of the reasons why people are hesitant to accept hand-gesture interfaces as an alternative input method is 
their lack of secondary feedback. Therefore, the focus of this study is on secondary feedback.</aside>
</section>

<section class="slide level1">
<p>The proposal</p>
<aside class="notes">
we propose using auditory feedback to complement visual feedback and make menu selection in gesture-based interfaces more efficient
</aside>
</section>

<section class="slide level1">
<p>Gesture Interface Example</p>
	<section class="slide level1">
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://millsjb.github.io/pres/Effects_of_Auditory_Feedback/ex1.png" 
		alt="Ref image" style="width:80%;height:550px;">
	</a>
	</section>
<aside class="notes">
This gesture interface uses two simple hand gestures, as in Figure 1b. Users can browse the menu with circular motions of the hand; the touching gesture of moving the hand forward selects the menu item</aside>
</section>
</section>

<section class="slide level1">
<p>Article's Aim</p>
<aside class="notes">
This article aims to study how users can access the circular menu with hand gestures and how much auditory feedback 
can affect the usability of menu selection in hand-gesture interfaces
</aside>
</section>

<section class="slide level1">
<p>How to increase richness in tactile cues/warnings?</p>
<section class="slide level1">
	</section>
	
	<section class="slide level1">
	<p>Vary parameters such as rhythm, roughness and intensity</p>
	</section>
<aside class="notes">
Varying parameters such as rhythm and roughness (amplitude modulation on the original waveform that provides the vibration) 
of such messages enabled the design of richer cues without cost in their recognition accuracy, as long as a reduced number 
of different levels for roughness was used.
</aside>
</section>

<section class="slide level1">
<p>In this study..</p>
<aside class="notes">
In this study, tactile cues are used along with audio, varying the rhythm of the cues, by imitating the rhythm of speech
</aside>
</section>

<section class="slide level1">
<a href="#">
	<img src="http://millsjb.github.io/pres/Speech_Tactons/Speech_tacton_ex.png" alt="Ref image" style="width:80%;height:550px;">
</a>
<aside class="notes">
exemplified here
</aside>
</section>

<section class="slide level1">
<p>Three urgency levels:</p>
	<section class="slide level1">
	</section>
	
	<section class="slide level1">
	<p>Level High (LH)</p>
	</section>
	
	<section class="slide level1">
	<p>Level Medium (LM)</p>
	</section>
	
	<section class="slide level1">
	<p>Level Low (LL)</p>
	</section>
<aside class="notes">
The words “Danger!”, “Warning!”, “Notice!” were added before each Lh, Lm and Ll. the ac- tor was instructed 
to speak messages of LH in an urgent manner, as if a loved one was in imminent danger. Accord- ingly, LM messages 
were spoken non-urgently, as if in a friendly conversation with nothing interesting about the situation and LL messages 
were spoken in a monotone, deadpan manner 
</aside>
</section>

<section class="slide level1">
<p>Experiment 1</p>
<aside class="notes">
The first experiment investigated the subjective responses provided by participants when exposed to the warnings.
</aside>
</section>

<section class="slide level1">
<p>A 6×3×4 within subjects design was used with Message, Modality and Design as the independent variables and Perceived Urgency (PU), Perceived Annoy- ance (PA) and Perceived Alerting Effectiveness (PAE) as the dependent ones</p>
<aside class="notes">
A 6×3×4 within subjects design was used with Message, Modality and Design as the independent variables and Perceived Urgency (PU), Perceived Annoy- ance (PA) and Perceived Alerting Effectiveness (PAE) as the dependent ones
</aside>
</section>

<section class="slide level1">
<p>Set-up</p>
<aside class="notes">
participants were exposed to the 54 cues (6 A, 24 T, 24 AT) in a random order, to familiarize them with the signals. Afterwards, they were again presented with the cues and asked to rate them all in terms of PA, PU and PAE, by 
completing a 5-point Likert scale for each rating
</aside>
</section>

<section class="slide level1">
<p>Results</p>
	<section class="slide level1">
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://millsjb.github.io/pres/Speech_Tactons/aa.png" 
		alt="Ref image" style="width:80%;height:550px;">
	</a>
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://millsjb.github.io/pres/Speech_Tactons/bb.png" 
		alt="Ref image" style="width:80%;height:550px;">
	</a>
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://millsjb.github.io/pres/Speech_Tactons/cc.png" 
		alt="Ref image" style="width:80%;height:550px;">
	</a>
	</section>
<aside class="notes">
Contrasts revealed that the significant differences in ratings of PU described above were not present in modality T
</aside>
</section>

<section class="slide level1">
<p>From Experiment 1 it was clear that AT was rated higher in all measures compared to A and T, showing clear evidence of the usefulness of the designed cues when modalities were combined</p>
<aside class="notes">
Further, it was evident that the urgency designed in the warnings was reflected in their PU. PAE escalated according to PU, indicating that messages signi- fying situations of higher importance were regarded as more useful
</aside>
</section>

<section class="slide level1">
<p>PA was higher for messages of both LH and LL and lower for LM</p>
<aside class="notes">
a further indi- cation of the higher utility of the cues when presented mul- timodally
</aside>
</section>

<section class="slide level1">
<p>Experiment 2</p>
<aside class="notes">
Experiment 2 investigated the recognition accuracy of participants when exposed to the T warnings
</aside>
</section>

<section class="slide level1">
<p>Results</p>
	<section class="slide level1">
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://millsjb.github.io/pres/Speech_Tactons/ex2.png" 
		alt="Ref image" style="width:80%;height:550px;">
	</a>
	</section>
<aside class="notes">
The results of PU are a clear indication that the messages de- signed conveyed the desired urgency. AT messages had higher 
ratings of PU, which is an improvement compared to A or T messages, when a situa- tion of high criticality needs to be 
conveyed</aside>
</section>

<section class="slide level1">
<p>This study shows that tactile cues can improve responses to speech warnings. This adds to the existing body of work, 
suggesting en- hanced responses to multimodal signals versus unimodal ones</p>
<aside class="notes">
T messages did not present highly differ- ent ratings of PU, adding to the argument that such cues work better when used multimodally
</aside>
</section>

<section class="slide level1">
<p>In terms of Tactile designs..</p>
<aside class="notes">
In terms of tactile designs, it was clear that intensity was the main factor that led to higher PU
</aside>
</section>

<section class="slide level1">
<p>In terms of Tactile designs..</p>
<aside class="notes">
This strengthens the evidence that intensity of the tactile part is useful to create more urgent messages and can be 
compared with [2,3,9], where high intensity of audio af- fected PU ratings</aside>
</section>

<section class="slide level1">
<p>Results</p>
	<section class="slide level1">
	</section>
	
	<section class="slide level1">
	<a href="#">
		<img src="http://millsjb.github.io/pres/Speech_Tactons/ex3.png" alt="Ref image" style="width:80%;height:550px;">
	</a>
	</section>
<aside class="notes">
Results showed that the addition of these new cues improved subjective responses of drivers to speech warnings. 
The warnings were clearly distinguished in terms of urgency, their annoyance was low and their alerting effectiveness 
changed similarly to urgency, increasing for more urgent messages and for multimodal cuesWe suggest the use of Speech 
Tactons to accompany speech warnings, so as to make use of the observed ad- vantages of multimodal cues, but not for 
low urgency situations, to avoid annoyance
</aside>
</section>

<section class="slide level1">
<p>Common Themes/What I took away from this..</p>
<aside class="notes">
Multi-modality is king, and perceived annoyance is actually a positive, but only when considering Lh scenarios
</aside>
</section>




			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				overview: true,
				touch: true,
			

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				
				// USE THESE SETTINGS FOR THE AUDIO AND AUTO-ADVANCE
				//autoSlide: 5000,
				audioPrefix: 'audio/',        // audio files are stored in the "audio" folder
				audioSuffix: '.ogg',	      // audio files have the ".ogg" ending
				audioDefaultDuration: 2, // default duration if no audio is available
				audioPlayerOpacity: 0.05,     // opacity value of audio player if unfocused
				audioTextToSpeechURL: "http://tts-api.com/tts.mp3?q=",  // text is added to this URL
				
				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } }
				]
			});
		</script>

	</body>
</html>